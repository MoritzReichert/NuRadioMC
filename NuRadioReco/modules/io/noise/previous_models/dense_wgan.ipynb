{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "layers = keras.layers\n",
    "tf.compat.v1.disable_eager_execution()  # gp loss won't work with eager\n",
    "from functools import partial\n",
    "from NuRadioReco.utilities import fft\n",
    "from NuRadioReco.utilities import units\n",
    "from NuRadioReco.framework import base_trace\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/lustre/fs22/group/radio/dhjelm/')\n",
    "import data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from file and shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('/lustre/fs22/group/radio/dhjelm/data.npy')\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS\n",
    "rms = data_preprocessing.rms_preprocessing(dataset)\n",
    "\n",
    "# L1\n",
    "l1 = data_preprocessing.l1_preprocessing(rms)\n",
    "\n",
    "# Remove DC-offset\n",
    "fft_traces = fft.time2freq(l1, 3.2*units.GHz)\n",
    "no_offset = fft_traces[:,1:len(fft_traces[0])]\n",
    "data_no_offset = fft.freq2time(no_offset, 3.2*units.GHz)\n",
    "\n",
    "# Shorten the trace\n",
    "short = data_no_offset[:,0:256]\n",
    "\n",
    "# Normalization\n",
    "normalize = data_preprocessing.normalize(short)\n",
    "\n",
    "# Set data to the normailzed data\n",
    "data = normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of the trace \n",
    "trace_length = len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on FFT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = abs(fft.time2freq(data, 3.2*units.GHz))\n",
    "# data = data[:,1:len(data[0])]\n",
    "# trace_length = len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a couple of traces to check that everything looks okey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time domain\n",
    "for i in range(5):\n",
    "    plt.plot(data[i])\n",
    "plt.title(\"Time domain\")\n",
    "plt.show()\n",
    "\n",
    "# Frequency domain\n",
    "for i in range(5):\n",
    "    plt.plot(abs(fft.time2freq(data[i], 3.2*units.GHz)))\n",
    "plt.title(\"Frequency domain\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator architecture\n",
    "def generator_model(latent_size, number_of_outputs):\n",
    "    \"\"\" Generator network \"\"\"\n",
    "    model = keras.models.Sequential(name=\"Generator\")\n",
    "\n",
    "    model.add(layers.Dense(latent_size, input_dim=latent_size, kernel_initializer='he_uniform'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.1)) \n",
    "\n",
    "    model.add(layers.Dense(latent_size*2, kernel_initializer='he_uniform'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.1)) \n",
    "\n",
    "    model.add(layers.Dense(latent_size*4, kernel_initializer='he_uniform'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "    model.add(layers.Dense(number_of_outputs))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "latent_size = 128\n",
    "g = generator_model(latent_size, trace_length)\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic (Discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic architecture\n",
    "def critic_model(trace_length):\n",
    "    model = keras.models.Sequential(name=\"Critic\")\n",
    "    \n",
    "    model.add(layers.Dense(128, kernel_initializer='he_uniform',\n",
    "  input_dim=trace_length))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.2)) \n",
    "    \n",
    "    model.add(layers.Dense(64, kernel_initializer='he_uniform'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.15)) \n",
    "    \n",
    "    model.add(layers.Dense(32, kernel_initializer='he_uniform'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.1)) \n",
    "    \n",
    "    model.add(layers.Dense(16, kernel_initializer='he_uniform'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.05)) \n",
    "    \n",
    "    model.add(layers.Dense(1)) # No activation!\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the critic\n",
    "critic = critic_model(trace_length)\n",
    "critic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines between the networks to be able to construct and train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable(model, trainable):\n",
    "    ''' Freezes/unfreezes the weights in the given model '''\n",
    "    for layer in model.layers:\n",
    "\n",
    "        if type(layer) is layers.BatchNormalization:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the critic during the generator training and unfreeze the generator during the generator training\n",
    "make_trainable(critic, False) \n",
    "make_trainable(g, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the generator o top of the critic and finiliaze the training pipeline of the generator\n",
    "gen_input = g.inputs\n",
    "generator_training = keras.models.Model(gen_input, critic(g(gen_input)))\n",
    "generator_training.summary()\n",
    "keras.utils.plot_model(generator_training, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\"Calculates the Wasserstein loss - critic maximises the distance between its output for real and generated samples.\n",
    "    To achieve this generated samples have the label -1 and real samples the label 1. Multiplying the outputs by the labels results to the wasserstein loss via the Kantorovich-Rubinstein duality\"\"\"\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "# Compile generator for training using the Wasserstein loss as loss function\n",
    "generator_training.compile(keras.optimizers.Adam(\n",
    "    0.0001, beta_1=0.5, beta_2=0.9, decay=0.0), loss=[wasserstein_loss])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the Wasserstein distance, we have to use the gradient penalty to enforce the Lipschitz constraint.\n",
    "Therefore, we need to design a layer that samples on straight lines between reals and fakes samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the batches used in training\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class UniformLineSampler(tf.keras.layers.Layer):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        weights = K.random_uniform((self.batch_size, 1))\n",
    "        return(weights * inputs[0]) + ((1 - weights) * inputs[1])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_trainable(critic, True)  # unfreeze the critic during the critic training\n",
    "make_trainable(g, False)  # freeze the generator during the critic training\n",
    "\n",
    "g_out = g(g.inputs)\n",
    "critic_out_fake_samples = critic(g_out)\n",
    "critic_out_data_samples = critic(critic.inputs)\n",
    "averaged_batch = UniformLineSampler(BATCH_SIZE)([g_out, critic.inputs[0]])\n",
    "averaged_batch_out = critic(averaged_batch)\n",
    "\n",
    "critic_training = keras.models.Model(inputs=[g.inputs, critic.inputs], outputs=[critic_out_fake_samples, critic_out_data_samples, averaged_batch_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_training.summary()\n",
    "keras.utils.plot_model(critic_training, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient penalty loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_batch, penalty_weight):\n",
    "    \"\"\"Calculates the gradient penalty.\n",
    "    The 1-Lipschitz constraint of improved WGANs is enforced by adding a term that penalizes a gradient norm in the critic unequal to 1.\"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_batch)\n",
    "    gradients_sqr_sum = K.sum(K.square(gradients)[0], axis=(1))\n",
    "    gradient_penalty = penalty_weight * K.square(1 - K.sqrt(gradients_sqr_sum))\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "# Construct the gradient penalty\n",
    "gradient_penalty_weight = 5\n",
    "gradient_penalty = partial(gradient_penalty_loss, averaged_batch=averaged_batch, penalty_weight=gradient_penalty_weight)  \n",
    "gradient_penalty.__name__ = 'gradient_penalty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile critic\n",
    "critic_training.compile(keras.optimizers.Adam(0.00005, beta_1=0.5, beta_2=0.9, decay=0.0), loss=[wasserstein_loss, wasserstein_loss, gradient_penalty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras throws an error when calculating a loss without having a label -> needed for using the gradient penalty loss\n",
    "positive_y = np.ones(BATCH_SIZE)\n",
    "negative_y = -positive_y\n",
    "dummy = np.zeros(BATCH_SIZE) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for generator and critic loss\n",
    "generator_loss = []\n",
    "critic_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "nsamples = len(data)\n",
    "critic_iterations = 5\n",
    "iterations_per_epoch = nsamples*4//(BATCH_SIZE*critic_iterations)\n",
    "iters = 0\n",
    "print(iterations_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(\"Epoch: \", epoch)\n",
    "\n",
    "    for iteration in range(iterations_per_epoch):\n",
    "       \n",
    "        for j in range(critic_iterations):\n",
    "            \n",
    "            # Pick data in batches and generate noise\n",
    "            bunch=data[BATCH_SIZE*(j+iteration):BATCH_SIZE*(j++iteration+1)]\n",
    "            noise_batch = np.random.randn(BATCH_SIZE, latent_size)\n",
    "       \n",
    "            \n",
    "            # Train critic\n",
    "            critic_loss.append(critic_training.train_on_batch([noise_batch, bunch], [negative_y, positive_y, dummy]))\n",
    "        \n",
    "\n",
    "        # Generate noise batch for generator\n",
    "        noise_batch = np.random.randn(BATCH_SIZE, latent_size)\n",
    "        \n",
    "        # Train the generator\n",
    "        generator_loss.append(generator_training.train_on_batch([noise_batch], [positive_y]))  \n",
    "        iters+=1\n",
    "        \n",
    "        # Printing errors and plotting example traces\n",
    "        if iters % 300 == 1:\n",
    "            print(\"Iteration\", iters)\n",
    "            print(\"Critic loss:\", critic_loss[-1])\n",
    "            print(\"Generator loss:\", generator_loss[-1])\n",
    "            \n",
    "            # Generate signals\n",
    "            generated_signals = g.predict_on_batch(np.random.randn(BATCH_SIZE, latent_size))\n",
    "            print(np.shape(generated_signals[0]))\n",
    "            print(len(abs(fft.time2freq(generated_signals[0], 3.2*units.GHz))))\n",
    "            \n",
    "            \n",
    "            # Plot data\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "            fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "            ax1.title.set_text('Time domain')\n",
    "            ax2.title.set_text('Frequency domain')\n",
    "            \n",
    "            \n",
    "\n",
    "            ax1.plot(data[0], label = \"Example trace\")\n",
    "#             plt.plot(np.random.randn(trace_length), label = 'Random noise')\n",
    "            for i in range(3):\n",
    "                ax1.plot(generated_signals[i], alpha=0.5)\n",
    "            \n",
    "            \n",
    "            # Plot frequency\n",
    "            ax2.plot(abs(fft.time2freq(data[0], 3.2*units.GHz)), label = \"Example trace\")\n",
    "            for i in range(3):\n",
    "                ax2.plot(abs(fft.time2freq(generated_signals[i], 3.2*units.GHz)),alpha=0.5)\n",
    "                \n",
    "            ax1.legend()\n",
    "            ax2.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "        #generated_signal = g.predict_on_batch(np.random.randn(BATCH_SIZE, latent_size))\n",
    "    \n",
    "    #print(\"Critic loss:\", critic_loss[-1])\n",
    "    #print(\"Generator loss:\", generator_loss[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss = np.array(critic_loss)\n",
    "plt.subplots(1, figsize=(10, 5))\n",
    "plt.plot(np.arange(len(critic_loss)), critic_loss[:,0], color='red', markersize=12, label=r'Total')\n",
    "plt.plot(np.arange(len(critic_loss)), critic_loss[:,1] + critic_loss[:, 2], color='green', label=r'Wasserstein', linestyle='dashed')\n",
    "plt.plot(np.arange(len(critic_loss)), critic_loss[:, 3], color='royalblue', markersize=12, label=r'Gradient penalty', linestyle='dashed')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(r'Iterations')\n",
    "plt.ylabel(r'Critic Loss')\n",
    "#plt.ylim(-6, 3)\n",
    "\n",
    "generator_loss = np.array(generator_loss)\n",
    "\n",
    "plt.subplots(1, figsize=(10, 5))\n",
    "plt.plot(np.arange(len(generator_loss)), generator_loss, color='red', markersize=12, label=r'Total')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(r'Iterations')\n",
    "plt.ylabel(r'Loss')\n",
    "\n",
    "\n",
    "# #save the generated networks\n",
    "# g.save('generator')\n",
    "# critic.save('critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_signals = g.predict_on_batch(np.random.randn(BATCH_SIZE, latent_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean generated: {np.mean(generated_signals)}\")\n",
    "print(f\"Mean data: {np.mean(data)}\\n\")\n",
    "\n",
    "print(f\"Std generated: {np.std(generated_signals)}\")\n",
    "print(f\"Std data: {np.std(data)}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.dpi\"]= 100\n",
    "\n",
    "plt.plot(data[0], label = \"Data\")\n",
    "for i in range(2):\n",
    "    plt.plot(generated_signals[i],alpha=0.5)\n",
    "# plt.plot(np.random.randn(trace_length), label = 'Random noise')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.dpi\"]= 100\n",
    "\n",
    "plt.plot(data[0], label = \"Example data\")\n",
    "for i in range(10):\n",
    "    plt.plot(generated_signals[5*i],alpha=0.2)\n",
    "# plt.plot(np.random.randn(trace_length), label = 'Random noise')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs(fft.time2freq(data[1], 3.2*units.GHz)), label = \"Data\")\n",
    "plt.plot(abs(fft.time2freq(generated_signals[0], 3.2*units.GHz)), label = \"Generated\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get frequencies of data\n",
    "data_freq = fft.time2freq(data, 3.2*units.GHz)\n",
    "\n",
    "print(nsamples)\n",
    "# Get frequencies of generated data\n",
    "generated_signals = g.predict_on_batch(np.random.randn(nsamples, latent_size))\n",
    "generator_freq = fft.time2freq(generated_signals, 3.2*units.GHz)\n",
    "\n",
    "# Get average frequencies for both\n",
    "avg_freq_data = np.mean(abs(data_freq), axis=0)\n",
    "avg_freq_generator = np.mean(abs(generator_freq), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Create dummy trace to get frequencies\n",
    "dummy_trace = base_trace.BaseTrace()\n",
    "dummy_trace.set_trace(np.zeros(trace_length), sampling_rate = 3.2*units.GHz)\n",
    "\n",
    "plt.plot(dummy_trace.get_frequencies()/units.MHz,avg_freq_data, label =f\"Data\")\n",
    "plt.plot(dummy_trace.get_frequencies()/units.MHz,avg_freq_generator, label =f\"Generator\")\n",
    "plt.xlabel(\"Frequency [MHz]\")\n",
    "# plt.ylabel(\"Square root of power per MHZ\")\n",
    "plt.title(\"Average frequencies for data and the generated data\")\n",
    "plt.legend()\n",
    "\n",
    "#plt.semilogy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_signals = g.predict_on_batch(np.random.randn(nsamples, latent_size))\n",
    "print(np.shape(generated_signals))\n",
    "print(np.shape(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
